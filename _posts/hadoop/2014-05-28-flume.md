---
layout: post
category : hadoop
tags : [flume]
---
{% include JB/setup %}

##Flume 是什么

Flume 有点像 scribe ，一个不错的日志分发工具，比如可以把日志直接从线上机器，copy到hdfs集群。

同事的反馈是 scribe 编译无比的麻烦，配置相当简单。

而 Flume 安装很轻松，配置相当的麻烦。


##Flume 安装

安装无比轻松，直接下载，然后解压就完事。


##Flume 几个重要的概念


###Event

Event 的定义比较宽广，如 新增加一个文件，文件append，文件MV 之类的，都可以定义为一个Event。



###Source

数据源，也就是定义一个什么样的数据源，比如监听一个端口、或者某一个目录、某一个文件之类的。

Flume 自带的有 Avro,Thrift,Exec,JMX,Spooling Directory,Twitter 1%, NetCat,Sequence Generator,Syslog Sources,HTTP Source

用的比较多的是 Exec，Spooling Directory, NetCat,Syslog,Http Source.


###Channel

可以理解为linux下的管道，不过linux 下的管道不能存东西，而Channel是可以保存数据的，也就是Source产生数据后，会存到Channel里面

Channel有很多种，比如基于内存的，基于文件的。

    Memory Channel #基于内存的，在内存缓种
    JDBC Channel #写入数据库
    File Channel #写入本地文件
    Spillable Memory Channel #有点像mapreduce 的spill，当内存写到一定程度的时候，写到磁盘。
    Pseudo Transaction Channel #这个还只能在测试环境用
    Custom Channel #这个可以自己Diy

###Sink

Sink 为数据保存的方式,现有的Sink分为如下几类

    HDFS Sink #写入Hdfs
    Logger Sink 
    Avro Sink
    Thrift Sink
    IRC Sink
    File Roll Sink #写入本地文件系统
    Null Sink #相当于linux  /dev/null
    HBaseSinks #写入Hbase
    MorphlineSolrSink #写入solr
    ElasticSearchSink #写入es
    Kite Dataset Sink 
    Custom Sink

### Flume  Interceptors 

Interceptors可以在中间过滤掉一些信息，同时也可以传递一些原信息，个人感觉叫 Filter会更好一点。

主要分为如下

    Timestamp Interceptor
    Host Interceptor
    Static Interceptor
    UUID Interceptor
    Morphline Interceptor
    Regex Filtering Interceptor
    Regex Extractor Interceptor




    

