---
layout: post
category : hadoop
tags : [flume]
---
{% include JB/setup %}

##Flume 是什么

Flume 有点像 scribe ，一个不错的日志分发工具，比如可以把日志直接从线上机器，copy到hdfs集群。

同事的反馈是 scribe 编译无比的麻烦，配置相当简单。

而 Flume 安装很轻松，配置相当的麻烦。


##Flume 安装

安装无比轻松，直接下载，然后解压就完事。


##Flume 几个重要的概念


###Event

Event 的定义比较宽广，如 新增加一个文件，文件append，文件MV 之类的，都可以定义为一个Event。



###Source

数据源，也就是定义一个什么样的数据源，比如监听一个端口、或者某一个目录、某一个文件之类的。

Flume 自带的有 Avro,Thrift,Exec,JMX,Spooling Directory,Twitter 1%, NetCat,Sequence Generator,Syslog Sources,HTTP Source

用的比较多的是 Exec，Spooling Directory, NetCat,Syslog,Http Source.


###Channel

可以理解为linux下的管道，不过linux 下的管道不能存东西，而Channel是可以保存数据的，也就是Source产生数据后，会存到Channel里面

Channel有很多种，比如基于内存的，基于文件的。

    Memory Channel #基于内存的，在内存缓种
    JDBC Channel #写入数据库
    File Channel #写入本地文件
    Spillable Memory Channel #有点像mapreduce 的spill，当内存写到一定程度的时候，写到磁盘。
    Pseudo Transaction Channel #这个还只能在测试环境用
    Custom Channel #这个可以自己Diy

###Sink

Sink 为数据保存的方式,现有的Sink分为如下几类

    HDFS Sink #写入Hdfs
    Logger Sink 
    Avro Sink
    Thrift Sink
    IRC Sink
    File Roll Sink #写入本地文件系统
    Null Sink #相当于linux  /dev/null
    HBaseSinks #写入Hbase
    MorphlineSolrSink #写入solr
    ElasticSearchSink #写入es
    Kite Dataset Sink 
    Custom Sink

### Flume  Interceptors 

Interceptors可以在中间过滤掉一些信息，同时也可以传递一些原信息，个人感觉叫 Filter会更好一点。

主要分为如下

    Timestamp Interceptor
    Host Interceptor
    Static Interceptor
    UUID Interceptor
    Morphline Interceptor
    Regex Filtering Interceptor
    Regex Extractor Interceptor



### Flume 注意项

    Exec Source 不能tail 整个目录或者正则匹配 如 tail -F  /a/b/c* 是错误的。
    Exec 可以对日志实时收集，但是不能保证不丢数据，因为Channel堵住，数据就丢了。
    Exec 如果tail的是一个软连接，如果把这个软连接重新连接，也会失败。

    SpoolSource 是监控目录下的新增加文件
    SpoolSource 传输完后，会重命名
    SpoolSource 不能包含子目录


### Flume配置

Flume Agent 配置, Flume 配置了一个 agent 收集日志

agent 必须有三个组件组成 source,channel,sink

agent 的名字为 ihive
agent 的source 类型为exec，他监控 log_current
agent 的sink 为avro监听的 10.1.15.199的这台机器
agent 的channel 为 memroy

    ihive_agent.sources = reader
    ihive_agent.channels = memoryChannel
    ihive_agent.sinks = avro-forward-sink

    ihive_agent.sources.reader.command =  tail -F /data/logs/log_current
    ihive_agent.sources.reader.logStdErr = true
    ihive_agent.sources.reader.restart = true
    ihive_agent.sources.reader.restartThrottle= 10000 
    ihive_agent.sources.reader.channels = memoryChannel

    ihive_agent.sinks.avro-forward-sink.type = avro
    ihive_agent.sinks.avro-forward-sink.hostname = 10.1.15.199
    ihive_agent.sinks.avro-forward-sink.port = 60001
    ihive_agent.sinks.avro-forward-sink.channel = memoryChannel

    ihive_agent.channels.memoryChannel.type = memory
    ihive_agent.channels.memoryChannel.capacity = 100000
    ihive_agent.channels.memoryChannel.transactionCapacity = 1000

Flume 配置一个写HDFS的Agent
如上面ihive_agent把数据写往了 10.1.15.199的机器，在199上面必须弄一个Agent，此ageng接收到数据，然后写入hdfs

    res_collector.sources = avro-collection-source
    res_collector.channels = memoryChannel
    res_collector.sinks = hdfs-sink

    res_collector.sources.avro-collection-source.type = avro
    res_collector.sources.avro-collection-source.bind = 0.0.0.0
    res_collector.sources.avro-collection-source.port = 60001
    res_collector.sources.avro-collection-source.interceptors = i1 i2 i3

    res_collector.sources.avro-collection-source.interceptors.i1.type = org.apache.flume.interceptor.HostInterceptor$Builder 
    res_collector.sources.avro-collection-source.interceptors.i1.preserveExisting = faluse 

    res_collector.sources.avro-collection-source.interceptors.i2.type = org.apache.flume.interceptor.TimestampInterceptor$Builder 

    res_collector.sources.avro-collection-source.interceptors.i3.type = static 
    res_collector.sources.avro-collection-source.interceptors.i3.key = datacenter
    res_collector.sources.avro-collection-source.interceptors.i3.value =Beijing

    res_collector.sources.avro-collection-source.channels = memoryChannel

    res_collector.sinks.hdfs-sink.type = hdfs
    res_collector.sinks.hdfs-sink.hdfs.path = hdfs://idc02-hd-ds-01:9000/user/www/udc/tmp/flume/audi
    res_collector.sinks.hdfs-sink.hdfs.writeFormat =Text 
    #res_collector.sinks.hdfs-sink.hdfs.fileType=DataStream #此类型无法压缩
    res_collector.sinks.hdfs-sink.hdfs.fileType=CompressedStream
    res_collector.sinks.hdfs-sink.hdfs.rollSize=0
    res_collector.sinks.hdfs-sink.hdfs.rollCount=0
    res_collector.sinks.hdfs-sink.hdfs.rollInterval=30 #多长时间切分一个文件
    res_collector.sinks.hdfs-sink.hdfs.codeC=gzip #压缩类型
    #res_collector.sinks.hdfs-sink.hdfs.filePrefix=%{filename}.%{host}.%Y-%m-%d
    #res_collector.sinks.hdfs-sink.hdfs.filePrefix=%{tailSrcFile}.%Y-%m-%d
    res_collector.sinks.hdfs-sink.hdfs.filePrefix=%{datacenter}.%{host}.%{file_base_name}.%Y-%m-%d
    #collector.sinks.hdfs-sink.serializer = avro_event
    #collector.sinks.hdfs-sink.fileType = DataStream
    res_collector.sinks.hdfs-sink.channel = memoryChannel
    res_collector.channels.memoryChannel.type = memory
    res_collector.channels.memoryChannel.capacity = 1000000


因为Memory Channel感觉不太靠普，所以最好能用 File Channel

###Flume配置 File Channel

    res_agent.sources = spooldirSource
    res_agent.channels = fileChannel
    res_agent.sinks = avro-forward-res

    res_agent.sources.spooldirSource.type=spooldir
    res_agent.sources.spooldirSource.spoolDir=/data/logs/access.log/
    res_agent.sources.spooldirSource.channels=fileChannel
    res_agent.sources.spooldirSource.ignorePattern=[^current]
    res_agent.sources.spooldirSource.fileHeader=true
    res_agent.sources.spooldirSource.fileHeaderKey=file_name
    res_agent.sources.spooldirSource.basenameHeader=true
    res_agent.sources.spooldirSource.consumeOrder=youngest
    res_agent.sources.spooldirSource.basenameHeaderKey=file_base_name
    res_agent.sources.spooldirSource.deserializer=org.apache.flume.sink.solr.morphline.BlobDeserializer$Builder
    res_agent.sources.spooldirSource.deserializer.maxBlobLength = 20000000
    res_agent.sources.spooldirSource.batchSize=1





    res_agent.sinks.avro-forward-res.type=avro
    res_agent.sinks.avro-forward-res.hostname=10.1.15.199
    res_agent.sinks.avro-forward-res.port=60001
    res_agent.sinks.avro-forward-res.channel = fileChannel 

    res_agent.channels.fileChannel.type=file
    res_agent.channels.fileChannel.checkpointDir=/home/rd/apache-flume-1.5.0-bin/data/c #配置检查点目录
    res_agent.channels.fileChannel.dataDirs=/home/rd/apache-flume-1.5.0-bin/data/d #配置数据目录



###启动flume 

启动ihive_agent 

    ./bin/flume-ng agent -n ihive_agent -c conf -f /home/rd/apache-flume-1.5.0-bin/conf/flume-conf.properties -Dflume.root.logger=INFO,console

启动 collect

./bin/flume-ng agent --conf /data/workspace/rd/workspace/apache-flume-1.5.0-bin/conf --conf-file /data/workspace/rd/workspace/apache-flume-1.5.0-bin/conf/flume-conf.properties --name res_collector  -Dflume.root.logger=INFO,console



[Flume 主页](https://flume.apache.org/ 'Flume 主页')

[trickle-feeding-webserver-log-files-to-hdfs-using-apache-flume](http://www.rittmanmead.com/2014/05/trickle-feeding-webserver-log-files-to-hdfs-using-apache-flume/ 'Flume')

[fluem demo](http://www.ibm.com/developerworks/opensource/library/bd-flumews/index.html 'flume demo')


